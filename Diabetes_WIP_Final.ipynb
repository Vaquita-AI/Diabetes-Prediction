{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "-CzFkWQK-Yry",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CzFkWQK-Yry",
        "outputId": "c4261fde-2e2a-4976-bf35-2243dcdcd647",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install catboost\n",
        "!pip install imblearn\n",
        "!pip install xgboost\n",
        "\n",
        "# Data Manipulation and Visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "\n",
        "# Utilities\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Google Colab and Paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = '/content/drive/My Drive/Diabetes Dataset/diabetes_dataset.csv'\n",
        "save_dir = '/content/drive/My Drive/Diabetes Dataset/optuna_studies'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1c5bee-e1df-496d-88fa-abffe71d5f80",
      "metadata": {
        "id": "0f1c5bee-e1df-496d-88fa-abffe71d5f80"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e275c83-093c-4c9f-9ebe-0956b0867f0c",
      "metadata": {
        "id": "9e275c83-093c-4c9f-9ebe-0956b0867f0c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path)\n",
        "# Binary encoding for gender and location\n",
        "binary_encoder = OneHotEncoder(drop='if_binary', sparse_output=False)\n",
        "binary_encoded = binary_encoder.fit_transform(df[['gender', 'location']])\n",
        "binary_encoded_df = pd.DataFrame(binary_encoded, columns=binary_encoder.get_feature_names_out(['gender', 'location']))\n",
        "df = pd.concat([df, binary_encoded_df], axis=1)\n",
        "df.drop(columns=['gender', 'location'], inplace=True)\n",
        "\n",
        "# Since smoking_history has multiple categories, we will use label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['smoking_history'] = label_encoder.fit_transform(df['smoking_history'])\n",
        "scaler = StandardScaler()\n",
        "num_features = ['age', 'bmi', 'hbA1c_level', 'blood_glucose_level']\n",
        "df[num_features] = scaler.fit_transform(df[num_features])\n",
        "\n",
        "# Split the features from the label\n",
        "X = df.drop(columns=['diabetes'])\n",
        "y = df['diabetes']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "smote = SMOTE()\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "# Remove the 'year', 'race', and 'location_' columns\n",
        "columns_to_remove = ['year']+ [col for col in X.columns if col.startswith('race:')] + [col for col in X.columns if col.startswith('location_')]\n",
        "X_train_cleaned = X_train_smote.drop(columns=columns_to_remove)\n",
        "X_test_cleaned = X_test.drop(columns=columns_to_remove)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "9Oo9Lbwf10Rm"
      },
      "id": "9Oo9Lbwf10Rm"
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = {\n",
        "    \"GaussianNB\": {\n",
        "        \"accuracy\": 0.75,\n",
        "        \"precision\": {\"class_0\": 0.99, \"class_1\": 0.24},\n",
        "        \"recall\": {\"class_0\": 0.73, \"class_1\": 0.94},\n",
        "        \"f1_score\": {\"class_0\": 0.84, \"class_1\": 0.39},\n",
        "        \"best_cv_score\": 0.8560346281260178,\n",
        "        \"best_params\": {'var_smoothing': 1e-06}\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        \"accuracy\": 0.93,\n",
        "        \"precision\": {\"class_0\": 0.98, \"class_1\": 0.54},\n",
        "        \"recall\": {\"class_0\": 0.94, \"class_1\": 0.79},\n",
        "        \"f1_score\": {\"class_0\": 0.96, \"class_1\": 0.64},\n",
        "        \"best_cv_score\": 0.9467395686567859,\n",
        "        \"best_params\": {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
        "    },\n",
        "    \"CatBoost\": {\n",
        "        \"accuracy\": 0.97,\n",
        "        \"precision\": {\"class_0\": 0.97, \"class_1\": 0.96},\n",
        "        \"recall\": {\"class_0\": 1.00, \"class_1\": 0.70},\n",
        "        \"f1_score\": {\"class_0\": 0.99, \"class_1\": 0.81},\n",
        "        \"best_cv_score\": 0.9769942405184306,\n",
        "        \"best_params\": {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"accuracy\": 0.96,\n",
        "        \"precision\": {\"class_0\": 0.98, \"class_1\": 0.79},\n",
        "        \"recall\": {\"class_0\": 0.98, \"class_1\": 0.74},\n",
        "        \"f1_score\": {\"class_0\": 0.98, \"class_1\": 0.76},\n",
        "        \"best_cv_score\": 0.9729655120292324,\n",
        "        \"best_params\": {'bootstrap': False, 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
        "    },\n",
        "    \"XGB\": {\n",
        "        \"accuracy\": 0.97,\n",
        "        \"precision\": {\"class_0\": 0.97, \"class_1\": 0.94},\n",
        "        \"recall\": {\"class_0\": 1.00, \"class_1\": 0.71},\n",
        "        \"f1_score\": {\"class_0\": 0.98, \"class_1\": 0.81},\n",
        "        \"best_cv_score\": 0.9760651261380138,\n",
        "        \"best_params\": {'colsample_bytree': 0.6448155608672209, 'gamma': 0.07957111980914833, 'learning_rate': 0.10694704332753689, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 233, 'reg_alpha': 0.09750671629451425, 'reg_lambda': 1.4907487792587846, 'subsample': 0.8891584386487102}\n",
        "    },\n",
        "    \"LogisticRegression\": {\n",
        "        \"accuracy\": 0.88,\n",
        "        \"precision\": {\"class_0\": 0.99, \"class_1\": 0.41},\n",
        "        \"recall\": {\"class_0\": 0.89, \"class_1\": 0.87},\n",
        "        \"f1_score\": {\"class_0\": 0.93, \"class_1\": 0.56},\n",
        "        \"best_cv_score\": 0.886275468881269,\n",
        "        \"best_params\": {'C': 0.01, 'solver': 'liblinear'}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ejF9cOwI11Oz"
      },
      "id": "ejF9cOwI11Oz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2f4af99b-8186-4220-a81b-a4b6709555c9",
      "metadata": {
        "id": "2f4af99b-8186-4220-a81b-a4b6709555c9"
      },
      "source": [
        "# 2. MODEL SHOWDOWN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4814b926-5310-4206-9944-f22e8dce366d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4814b926-5310-4206-9944-f22e8dce366d",
        "outputId": "3e80704a-4721-437d-e507-00533f3efbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.93     18300\n",
            "           1       0.41      0.87      0.56      1700\n",
            "\n",
            "    accuracy                           0.88     20000\n",
            "   macro avg       0.70      0.88      0.75     20000\n",
            "weighted avg       0.94      0.88      0.90     20000\n",
            "\n",
            "Best parameters found:  {'C': 0.01, 'solver': 'liblinear'}\n",
            "Best cross-validation score:  0.886275468881269\n"
          ]
        }
      ],
      "source": [
        "# Define the LogisticRegression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter grid\n",
        "log_reg_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=log_reg_param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_log_reg.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the parameter distribution\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': randint(100, 300),\n",
        "    'max_depth': randint(4, 8),\n",
        "    'learning_rate': uniform(0.01, 0.1),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4),\n",
        "    'gamma': uniform(0, 0.2),\n",
        "    'min_child_weight': randint(1, 5),\n",
        "    'reg_alpha': uniform(0, 0.1),\n",
        "    'reg_lambda': uniform(1, 1)\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=xgb_param_dist, n_iter=100, cv=5, n_jobs=-1, scoring='f1', random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "random_search_xgb.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_xgb = random_search_xgb.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_xgb = best_xgb.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search_xgb.best_params_)\n",
        "print(\"Best cross-validation score: \", random_search_xgb.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT8YchbHsBYM",
        "outputId": "2d19f180-f739-458f-d11e-22a922d3924d"
      },
      "id": "XT8YchbHsBYM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [21:25:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     18300\n",
            "           1       0.94      0.71      0.81      1700\n",
            "\n",
            "    accuracy                           0.97     20000\n",
            "   macro avg       0.95      0.85      0.90     20000\n",
            "weighted avg       0.97      0.97      0.97     20000\n",
            "\n",
            "Best parameters found:  {'colsample_bytree': 0.6448155608672209, 'gamma': 0.07957111980914833, 'learning_rate': 0.10694704332753689, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 233, 'reg_alpha': 0.09750671629451425, 'reg_lambda': 1.4907487792587846, 'subsample': 0.8891584386487102}\n",
            "Best cross-validation score:  0.9760651261380138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t2ygIWsqqI8j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2ygIWsqqI8j",
        "outputId": "dcff0501-4a8f-4572-ada8-ad3bc076c0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     18300\n",
            "           1       0.79      0.74      0.76      1700\n",
            "\n",
            "    accuracy                           0.96     20000\n",
            "   macro avg       0.88      0.86      0.87     20000\n",
            "weighted avg       0.96      0.96      0.96     20000\n",
            "\n",
            "Best parameters found:  {'bootstrap': False, 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Best cross-validation score:  0.9729655120292324\n"
          ]
        }
      ],
      "source": [
        "# Define the RandomForestClassifier model\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Define the extended parameter grid\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_rf.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0d8724-a059-4d2f-876f-8d9022997e46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0d8724-a059-4d2f-876f-8d9022997e46",
        "outputId": "b199cec3-cdb1-4c32-ef49-7e0ae8cb48f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     18300\n",
            "           1       0.96      0.70      0.81      1700\n",
            "\n",
            "    accuracy                           0.97     20000\n",
            "   macro avg       0.97      0.85      0.90     20000\n",
            "weighted avg       0.97      0.97      0.97     20000\n",
            "\n",
            "Best parameters found:  {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Best cross-validation score:  0.9769942405184306\n"
          ]
        }
      ],
      "source": [
        "# Define the CatBoostClassifier\n",
        "catboost = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "catboost_param_grid = {\n",
        "    'iterations': [100, 200],\n",
        "    'depth': [4, 6],\n",
        "    'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=catboost, param_grid=catboost_param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_catboost = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_catboost.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2516667-375e-4257-bcea-a9187610aca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2516667-375e-4257-bcea-a9187610aca9",
        "outputId": "5938e49e-1948-4336-e364-992be3abe8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     18300\n",
            "           1       0.54      0.79      0.64      1700\n",
            "\n",
            "    accuracy                           0.93     20000\n",
            "   macro avg       0.76      0.86      0.80     20000\n",
            "weighted avg       0.94      0.93      0.93     20000\n",
            "\n",
            "Best parameters found:  {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Best cross-validation score:  0.9467395686567859\n"
          ]
        }
      ],
      "source": [
        "# Define the KNeighborsClassifier model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the parameter grid\n",
        "knn_param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 10, 20, 40],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_knn.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f01838d-8d78-4377-bc53-aa6cdbe898c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f01838d-8d78-4377-bc53-aa6cdbe898c8",
        "outputId": "d0dc05d9-903f-41f1-edaf-698c4850b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.73      0.84     18300\n",
            "           1       0.24      0.94      0.39      1700\n",
            "\n",
            "    accuracy                           0.75     20000\n",
            "   macro avg       0.62      0.84      0.61     20000\n",
            "weighted avg       0.93      0.75      0.80     20000\n",
            "\n",
            "Best parameters found:  {'var_smoothing': 1e-06}\n",
            "Best cross-validation score:  0.8560346281260178\n"
          ]
        }
      ],
      "source": [
        "# Define the GaussianNB model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Define the parameter grid\n",
        "gnb_param_grid = {\n",
        "    'var_smoothing': np.logspace(-9, -6, 4)\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=gnb, param_grid=gnb_param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_cleaned, y_train_smote)\n",
        "\n",
        "# Get the best estimator\n",
        "best_gnb = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_gnb.predict(X_test_cleaned)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zDYTRqG18aV6",
      "metadata": {
        "id": "zDYTRqG18aV6"
      },
      "source": [
        "NEURAL NETS ON DRUGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0q4EGldC8cKU",
      "metadata": {
        "id": "0q4EGldC8cKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73073f0-c280-43e4-b69f-0864e30784ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-07-26 05:14:05,876] Using an existing study with name 'example_study' instead of creating a new one.\n",
            "[I 2024-07-26 05:18:09,415] Trial 44 finished with value: 0.5871913580246914 and parameters: {'hidden_dim': 96, 'n_layers': 3, 'dropout_rate': 0.12025313227122837, 'learning_rate': 0.004336429107743327, 'activation': 'Tanh', 'optimizer': 'SGD'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:22:08,865] Trial 45 finished with value: 0.6024566192240203 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.11603338633603141, 'learning_rate': 0.003182738519228813, 'activation': 'ReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:26:27,808] Trial 46 finished with value: 0.5576616474756421 and parameters: {'hidden_dim': 256, 'n_layers': 3, 'dropout_rate': 0.17126796848121623, 'learning_rate': 0.005836536400903046, 'activation': 'ReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:29:59,157] Trial 47 finished with value: 0.5442271074952398 and parameters: {'hidden_dim': 224, 'n_layers': 2, 'dropout_rate': 0.14149330129635435, 'learning_rate': 0.0071239079343497266, 'activation': 'LeakyReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:33:05,512] Trial 48 finished with value: 0.5272281275551921 and parameters: {'hidden_dim': 160, 'n_layers': 1, 'dropout_rate': 0.20345406545168948, 'learning_rate': 0.008097207572336416, 'activation': 'Tanh', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:37:19,647] Trial 49 finished with value: 0.5431078988125968 and parameters: {'hidden_dim': 32, 'n_layers': 4, 'dropout_rate': 0.40316844196663104, 'learning_rate': 0.0013557039548490976, 'activation': 'ReLU', 'optimizer': 'SGD'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:42:12,193] Trial 50 finished with value: 0.5848845867460909 and parameters: {'hidden_dim': 128, 'n_layers': 5, 'dropout_rate': 0.1455409848245987, 'learning_rate': 0.009470060790373675, 'activation': 'ReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:46:26,035] Trial 51 finished with value: 0.622069523039612 and parameters: {'hidden_dim': 96, 'n_layers': 3, 'dropout_rate': 0.10012084249542039, 'learning_rate': 0.0002442116392169739, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:50:51,798] Trial 52 finished with value: 0.45649152071170424 and parameters: {'hidden_dim': 192, 'n_layers': 4, 'dropout_rate': 0.3382368312883881, 'learning_rate': 0.0062075805522406975, 'activation': 'ReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:55:05,261] Trial 53 finished with value: 0.598428215449492 and parameters: {'hidden_dim': 64, 'n_layers': 3, 'dropout_rate': 0.18842135746951905, 'learning_rate': 0.0074013017607844215, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 05:59:17,573] Trial 54 finished with value: 0.591258675670606 and parameters: {'hidden_dim': 64, 'n_layers': 3, 'dropout_rate': 0.11824561434952434, 'learning_rate': 0.007682506538270404, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:03:03,385] Trial 55 finished with value: 0.5800074599030214 and parameters: {'hidden_dim': 64, 'n_layers': 2, 'dropout_rate': 0.1543019906329519, 'learning_rate': 0.008887199823635512, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:07:17,659] Trial 56 finished with value: 0.5747084548104956 and parameters: {'hidden_dim': 224, 'n_layers': 3, 'dropout_rate': 0.19390272430119612, 'learning_rate': 0.0028519167865104856, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:11:29,207] Trial 57 finished with value: 0.541144598713173 and parameters: {'hidden_dim': 256, 'n_layers': 3, 'dropout_rate': 0.13690957020397995, 'learning_rate': 0.008632562139016843, 'activation': 'Tanh', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:14:59,313] Trial 58 finished with value: 0.607565011820331 and parameters: {'hidden_dim': 224, 'n_layers': 2, 'dropout_rate': 0.1662704324435823, 'learning_rate': 0.0042912713254009355, 'activation': 'ReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:19:12,133] Trial 59 finished with value: 0.6485428447150935 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.11254013410427566, 'learning_rate': 0.0021562794328969107, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:23:07,688] Trial 60 finished with value: 0.5879235282983153 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.11576731499859211, 'learning_rate': 0.0021860511411555036, 'activation': 'LeakyReLU', 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:27:47,393] Trial 61 finished with value: 0.6161879895561357 and parameters: {'hidden_dim': 192, 'n_layers': 4, 'dropout_rate': 0.49414731896943226, 'learning_rate': 0.0006414647963097422, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:31:04,192] Trial 62 finished with value: 0.5547210300429184 and parameters: {'hidden_dim': 192, 'n_layers': 2, 'dropout_rate': 0.11370035806102362, 'learning_rate': 0.0015585498163051895, 'activation': 'LeakyReLU', 'optimizer': 'SGD'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:35:15,011] Trial 63 finished with value: 0.6363830240989551 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.13091535985959127, 'learning_rate': 0.0007486229362056, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:39:25,331] Trial 64 finished with value: 0.6342905405405406 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1306916951904973, 'learning_rate': 0.0006282776078900892, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:43:36,091] Trial 65 finished with value: 0.6500108624809907 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.13387579082307902, 'learning_rate': 0.0009631507234210717, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:47:47,163] Trial 66 finished with value: 0.6242011956297671 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.47350044079546366, 'learning_rate': 0.000789094282841827, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:51:58,470] Trial 67 finished with value: 0.6118160190325139 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.15676016459020675, 'learning_rate': 0.001982280764403473, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 06:56:11,044] Trial 68 finished with value: 0.6397248495270851 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.142119851124181, 'learning_rate': 0.0009027195426086659, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:00:24,935] Trial 69 finished with value: 0.6342905405405406 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1450657958359853, 'learning_rate': 0.0009072988198488001, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:04:36,465] Trial 70 finished with value: 0.5964437572477773 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.17080846230079408, 'learning_rate': 0.001100625763361612, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:08:20,813] Trial 71 finished with value: 0.5849831901382143 and parameters: {'hidden_dim': 192, 'n_layers': 2, 'dropout_rate': 0.29085047558011984, 'learning_rate': 0.0017876080868226485, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:12:32,943] Trial 72 finished with value: 0.6349070711386456 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1068116397859137, 'learning_rate': 0.00032782994808616217, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:16:45,227] Trial 73 finished with value: 0.6266556291390729 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.10976539494280398, 'learning_rate': 0.00018895156589717973, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:20:57,157] Trial 74 finished with value: 0.6492869875222816 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.100254240443442, 'learning_rate': 0.00041258119908792917, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:25:07,742] Trial 75 finished with value: 0.6275746111811686 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.12316998240346876, 'learning_rate': 0.0011488210501416122, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:29:18,102] Trial 76 finished with value: 0.6552631578947368 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.14212481941603172, 'learning_rate': 0.0005029278745429348, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:33:29,179] Trial 77 finished with value: 0.6341156747694887 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.15396135912986153, 'learning_rate': 0.001352850926371197, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:37:40,208] Trial 78 finished with value: 0.6147393648891551 and parameters: {'hidden_dim': 32, 'n_layers': 3, 'dropout_rate': 0.14078110748800368, 'learning_rate': 0.00039950709186063835, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:41:23,342] Trial 79 finished with value: 0.5814591332967635 and parameters: {'hidden_dim': 192, 'n_layers': 2, 'dropout_rate': 0.10176090186558558, 'learning_rate': 0.009539269909881814, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:45:34,314] Trial 80 finished with value: 0.6214343928280359 and parameters: {'hidden_dim': 160, 'n_layers': 3, 'dropout_rate': 0.18109268339811535, 'learning_rate': 0.0010847314609373202, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:49:44,718] Trial 81 finished with value: 0.5868376390722232 and parameters: {'hidden_dim': 128, 'n_layers': 3, 'dropout_rate': 0.13620067046325401, 'learning_rate': 0.0021981772525727374, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:53:28,215] Trial 82 finished with value: 0.6193707587908699 and parameters: {'hidden_dim': 192, 'n_layers': 2, 'dropout_rate': 0.12035784966074069, 'learning_rate': 0.0005142750935311351, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 07:57:38,955] Trial 83 finished with value: 0.5953966140384249 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1275540333238237, 'learning_rate': 1.7114806817662633e-05, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:01:50,018] Trial 84 finished with value: 0.6128580012016824 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1469077335156402, 'learning_rate': 0.001652829120614102, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:06:00,788] Trial 85 finished with value: 0.645119305856833 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.1648697480195313, 'learning_rate': 0.0008425285892108516, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:10:11,827] Trial 86 finished with value: 0.632514817950889 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.16201811402844501, 'learning_rate': 0.0009493420298895992, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:14:22,934] Trial 87 finished with value: 0.6483757682177349 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.11124975062257361, 'learning_rate': 0.001352329000735855, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:18:33,999] Trial 88 finished with value: 0.6145454545454545 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.10569875275499169, 'learning_rate': 0.001260901335121131, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:22:16,800] Trial 89 finished with value: 0.5819672131147542 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.17413030897611415, 'learning_rate': 0.0026151352566010644, 'activation': 'LeakyReLU', 'optimizer': 'SGD'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:26:27,962] Trial 90 finished with value: 0.6375952582557156 and parameters: {'hidden_dim': 192, 'n_layers': 3, 'dropout_rate': 0.11622011778054768, 'learning_rate': 0.0014088478174229643, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n",
            "[I 2024-07-26 08:29:41,208] Trial 91 finished with value: 0.6004663816556549 and parameters: {'hidden_dim': 256, 'n_layers': 1, 'dropout_rate': 0.12494561847753471, 'learning_rate': 0.0018480712109894453, 'activation': 'LeakyReLU', 'optimizer': 'Adam'}. Best is trial 24 with value: 0.6571815718157181.\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert the data to PyTorch tensors and move them to the GPU\n",
        "X_train_tensor = torch.tensor(X_train_cleaned.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train_smote.values, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_cleaned.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the neural network class\n",
        "class DiabetesNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout_rate, activation):\n",
        "        super(DiabetesNN, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(activation)\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(activation)\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [32 * i for i in range(1, 9)])  # Multiples of 32 up to 32*8\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
        "    activation_name = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'Tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
        "\n",
        "    # Activation function\n",
        "    if activation_name == 'ReLU':\n",
        "        activation = nn.ReLU()\n",
        "    elif activation_name == 'LeakyReLU':\n",
        "        activation = nn.LeakyReLU()\n",
        "    elif activation_name == 'Tanh':\n",
        "        activation = nn.Tanh()\n",
        "\n",
        "    # Initialize the model\n",
        "    model = DiabetesNN(input_dim=X_train_tensor.shape[1], hidden_dim=hidden_dim, output_dim=1, n_layers=n_layers, dropout_rate=dropout_rate, activation=activation).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(25):  # You can increase the number of epochs\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _ in test_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            preds = torch.round(torch.sigmoid(outputs))\n",
        "            y_pred.extend(preds.cpu().numpy())  # Move predictions back to CPU for evaluation\n",
        "\n",
        "    # Move y_test_tensor back to CPU for evaluation\n",
        "    f1 = f1_score(y_test_tensor.cpu().numpy(), y_pred)\n",
        "    return f1\n",
        "\n",
        "# Create a study object and optimize the objective function\n",
        "study = optuna.create_study(direction='maximize', storage=f'sqlite:///{save_dir}/example_study.db', study_name='example_study', load_if_exists=True)\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Save the study results to a file\n",
        "with open(os.path.join(save_dir, \"study.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(study, f)\n",
        "\n",
        "# Load the study results from a file\n",
        "with open(os.path.join(save_dir, \"study.pkl\"), \"rb\") as f:\n",
        "    study = pickle.load(f)\n",
        "\n",
        "# Display the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Function to resume the study\n",
        "def resume_study(study_name, storage):\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
        "    study.optimize(objective, n_trials=100)\n",
        "    return study\n",
        "\n",
        "# Resume the study\n",
        "study = resume_study(\"example_study\", f'sqlite:///{save_dir}/example_study.db')\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "best_params = study.best_params_\n",
        "activation = nn.ReLU() if best_params['activation'] == 'ReLU' else nn.LeakyReLU() if best_params['activation'] == 'LeakyReLU' else nn.Tanh()\n",
        "model = DiabetesNN(input_dim=X_train_tensor.shape[1], hidden_dim=best_params['hidden_dim'], output_dim=1, n_layers=best_params['n_layers'], dropout_rate=best_params['dropout_rate'], activation=activation).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate']) if best_params['optimizer'] == 'Adam' else optim.SGD(model.parameters(), lr=best_params['learning_rate']) if best_params['optimizer'] == 'SGD' else optim.RMSprop(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "model.train()\n",
        "for epoch in range(20):  # You can increase the number of epochs\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation of the final model\n",
        "model.eval()\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, _ in test_loader:\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        preds = torch.round(torch.sigmoid(outputs))\n",
        "        y_pred.extend(preds.cpu().numpy())  # Move predictions back to CPU for evaluation\n",
        "\n",
        "accuracy = accuracy_score(y_test_tensor.cpu().numpy(), y_pred)  # Move y_test_tensor back to CPU for evaluation\n",
        "f1 = f1_score(y_test_tensor.cpu().numpy(), y_pred)  # Move y_test_tensor back to CPU for evaluation\n",
        "print(\"Final Model Accuracy: \", accuracy)\n",
        "print(\"Final Model F1 Score: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1g1XOeJsNKbg",
      "metadata": {
        "id": "1g1XOeJsNKbg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "z0_eug3DMUVA",
      "metadata": {
        "id": "z0_eug3DMUVA"
      },
      "source": [
        "Train with best pars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "El18lrk-MT28",
      "metadata": {
        "id": "El18lrk-MT28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Manually input the best parameters\n",
        "hidden_dim = 82\n",
        "n_layers = 3\n",
        "dropout_rate = 0.44302401141611214\n",
        "learning_rate = 0.008185942513111595\n",
        "activation = nn.LeakyReLU()\n",
        "optimizer_name = 'RMSprop'\n",
        "\n",
        "# Initialize the model\n",
        "model = DiabetesNN(input_dim=X_train_tensor.shape[1], hidden_dim=hidden_dim, output_dim=1, n_layers=n_layers, dropout_rate=dropout_rate, activation=activation).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "elif optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "elif optimizer_name == 'RMSprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Early stopping and learning rate scheduler\n",
        "early_stopping_patience = 5\n",
        "lr_scheduler_patience = 3\n",
        "lr_scheduler_factor = 0.1\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_scheduler_factor, patience=lr_scheduler_patience, verbose=True)\n",
        "\n",
        "# Training loop with early stopping and learning rate reduction\n",
        "model.train()\n",
        "for epoch in range(200):  # You can increase the number of epochs\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.round(torch.sigmoid(outputs))\n",
        "        train_correct += (preds == y_batch).sum().item()\n",
        "        train_total += y_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.round(torch.sigmoid(outputs))\n",
        "            val_correct += (preds == y_batch).sum().item()\n",
        "            val_total += y_batch.size(0)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_accuracy = val_correct / val_total\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(os.path.join(save_dir, \"best_model.pth\")))\n",
        "\n",
        "# Evaluation of the final model\n",
        "model.eval()\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, _ in test_loader:\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        preds = torch.round(torch.sigmoid(outputs))\n",
        "        y_pred.extend(preds.cpu().numpy())  # Move predictions back to CPU for evaluation\n",
        "\n",
        "accuracy = accuracy_score(y_test_tensor.cpu().numpy(), y_pred)  # Move y_test_tensor back to CPU for evaluation\n",
        "f1 = f1_score(y_test_tensor.cpu().numpy(), y_pred)  # Move y_test_tensor back to CPU for evaluation\n",
        "print(\"Final Model Accuracy: \", accuracy)\n",
        "print(\"Final Model F1 Score: \", f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
